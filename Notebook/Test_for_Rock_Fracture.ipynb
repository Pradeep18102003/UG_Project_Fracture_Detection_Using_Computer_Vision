{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting Path (Encoder)\n",
        "        self.inc = DoubleConv(in_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
        "\n",
        "        # Expansive Path (Decoder)\n",
        "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.conv1 = DoubleConv(1024, 512)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv2 = DoubleConv(512, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv3 = DoubleConv(256, 128)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv4 = DoubleConv(128, 64)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        up_x1 = self.up1(x5)\n",
        "        # Handle potential size mismatch from pooling\n",
        "        if up_x1.shape != x4.shape:\n",
        "            up_x1 = TF.resize(up_x1, size=x4.shape[2:])\n",
        "        concat1 = torch.cat([x4, up_x1], dim=1)\n",
        "        dec_x1 = self.conv1(concat1)\n",
        "\n",
        "        up_x2 = self.up2(dec_x1)\n",
        "        if up_x2.shape != x3.shape:\n",
        "            up_x2 = TF.resize(up_x2, size=x3.shape[2:])\n",
        "        concat2 = torch.cat([x3, up_x2], dim=1)\n",
        "        dec_x2 = self.conv2(concat2)\n",
        "\n",
        "        up_x3 = self.up3(dec_x2)\n",
        "        if up_x3.shape != x2.shape:\n",
        "            up_x3 = TF.resize(up_x3, size=x2.shape[2:])\n",
        "        concat3 = torch.cat([x2, up_x3], dim=1)\n",
        "        dec_x3 = self.conv3(concat3)\n",
        "\n",
        "        up_x4 = self.up4(dec_x3)\n",
        "        if up_x4.shape != x1.shape:\n",
        "            up_x4 = TF.resize(up_x4, size=x1.shape[2:])\n",
        "        concat4 = torch.cat([x1, up_x4], dim=1)\n",
        "        dec_x4 = self.conv4(concat4)\n",
        "\n",
        "        # Final output\n",
        "        logits = self.outc(dec_x4)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "S5KMuwaNLcmX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "_iSrRLCkLAJv",
        "outputId": "bdc6a10b-9118-4df6-e6f5-357016407837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded image from URL.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4279754592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                     warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_PATH = \"/content/unet_model.pth\"       # Path to your saved model\n",
        "\n",
        "# URL of the fracture image you want to test\n",
        "IMAGE_URL = \"https://assets.spe.org/dims4/default/16bd6f8/2147483647/strip/true/crop/1024x688+0+0/resize/800x538!/quality/90/?url=http%3A%2F%2Fspe-brightspot.s3.us-east-2.amazonaws.com%2F92%2F73%2Fa8daef254e94819d017c1dfee840%2Fjpt-2023-04-geothermallead.jpg\" # Example X-ray image URL\n",
        "\n",
        "# --- Step 1: Download the image from the URL ---\n",
        "\n",
        "try:\n",
        "    response = requests.get(IMAGE_URL)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    image_bytes = io.BytesIO(response.content)\n",
        "    original_image = Image.open(image_bytes).convert(\"RGB\")\n",
        "    print(f\"Successfully downloaded image from URL.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading image: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 2: Load the trained model ---\n",
        "\n",
        "# Make sure your UNet class definition is available\n",
        "# from model import UNet\n",
        "\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Model loaded and in evaluation mode.\")\n",
        "\n",
        "# --- Step 3: Define transformations and preprocess the image ---\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "image_np = np.array(original_image)\n",
        "input_image = transform(image=image_np)[\"image\"]\n",
        "input_image = input_image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# --- Step 4: Make a prediction ---\n",
        "\n",
        "print(\"Making prediction...\")\n",
        "with torch.no_grad():\n",
        "    pred_logits = model(input_image)\n",
        "    pred_probs = torch.sigmoid(pred_logits)\n",
        "    pred_mask = (pred_probs > 0.5).float()\n",
        "\n",
        "# --- Step 5: Visualize the results ---\n",
        "\n",
        "output_probs = pred_probs.squeeze().cpu().numpy()\n",
        "output_mask = pred_mask.squeeze().cpu().numpy()\n",
        "\n",
        "print(\"Displaying results...\")\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "ax1.imshow(original_image)\n",
        "ax1.set_title(\"Original Image (from URL)\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "ax2.imshow(output_probs, cmap='gray')\n",
        "ax2.set_title(\"Predicted Probabilities\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "ax3.imshow(output_mask, cmap='gray')\n",
        "ax3.set_title(\"Final Binary Mask\")\n",
        "ax3.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efh5wROwLjo5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}